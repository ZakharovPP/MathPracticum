{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f709f9e4-2421-49b2-b73b-b53549a276c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\zahar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\zahar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\zahar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\zahar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/150\n",
      "Iteration 2/150\n",
      "Iteration 3/150\n",
      "Iteration 4/150\n",
      "Iteration 5/150\n",
      "Iteration 6/150\n",
      "Iteration 7/150\n",
      "Iteration 8/150\n",
      "Iteration 9/150\n",
      "Iteration 10/150\n",
      "Iteration 11/150\n",
      "Iteration 12/150\n",
      "Iteration 13/150\n",
      "Iteration 14/150\n",
      "Iteration 15/150\n",
      "Iteration 16/150\n",
      "Iteration 17/150\n",
      "Iteration 18/150\n",
      "Iteration 19/150\n",
      "Iteration 20/150\n",
      "Iteration 21/150\n",
      "Iteration 22/150\n",
      "Iteration 23/150\n",
      "Iteration 24/150\n",
      "Iteration 25/150\n",
      "Iteration 26/150\n",
      "Iteration 27/150\n",
      "Iteration 28/150\n",
      "Iteration 29/150\n",
      "Iteration 30/150\n",
      "Iteration 31/150\n",
      "Iteration 32/150\n",
      "Iteration 33/150\n",
      "Iteration 34/150\n",
      "Iteration 35/150\n",
      "Iteration 36/150\n",
      "Iteration 37/150\n",
      "Iteration 38/150\n",
      "Iteration 39/150\n",
      "Iteration 40/150\n",
      "Iteration 41/150\n",
      "Iteration 42/150\n",
      "Iteration 43/150\n",
      "Iteration 44/150\n",
      "Iteration 45/150\n",
      "Iteration 46/150\n",
      "Iteration 47/150\n",
      "Iteration 48/150\n",
      "Iteration 49/150\n",
      "Iteration 50/150\n",
      "Iteration 51/150\n",
      "Iteration 52/150\n",
      "Iteration 53/150\n",
      "Iteration 54/150\n",
      "Iteration 55/150\n",
      "Iteration 56/150\n",
      "Iteration 57/150\n",
      "Iteration 58/150\n",
      "Iteration 59/150\n",
      "Iteration 60/150\n",
      "Iteration 61/150\n",
      "Iteration 62/150\n",
      "Iteration 63/150\n",
      "Iteration 64/150\n",
      "Iteration 65/150\n",
      "Iteration 66/150\n",
      "Iteration 67/150\n",
      "Iteration 68/150\n",
      "Iteration 69/150\n",
      "Iteration 70/150\n",
      "Iteration 71/150\n",
      "Iteration 72/150\n",
      "Iteration 73/150\n",
      "Iteration 74/150\n",
      "Iteration 75/150\n",
      "Iteration 76/150\n",
      "Iteration 77/150\n",
      "Iteration 78/150\n",
      "Iteration 79/150\n",
      "Iteration 80/150\n",
      "Iteration 81/150\n",
      "Iteration 82/150\n",
      "Iteration 83/150\n",
      "Iteration 84/150\n",
      "Iteration 85/150\n",
      "Iteration 86/150\n",
      "Iteration 87/150\n",
      "Iteration 88/150\n",
      "Iteration 89/150\n",
      "Iteration 90/150\n",
      "Iteration 91/150\n",
      "Iteration 92/150\n",
      "Iteration 93/150\n",
      "Iteration 94/150\n",
      "Iteration 95/150\n",
      "Iteration 96/150\n",
      "Iteration 97/150\n",
      "Iteration 98/150\n",
      "Iteration 99/150\n",
      "Iteration 100/150\n",
      "Iteration 101/150\n",
      "Iteration 102/150\n",
      "Iteration 103/150\n",
      "Iteration 104/150\n",
      "Iteration 105/150\n",
      "Iteration 106/150\n",
      "Iteration 107/150\n",
      "Iteration 108/150\n",
      "Iteration 109/150\n",
      "Iteration 110/150\n",
      "Iteration 111/150\n",
      "Iteration 112/150\n",
      "Iteration 113/150\n",
      "Iteration 114/150\n",
      "Iteration 115/150\n",
      "Iteration 116/150\n",
      "Iteration 117/150\n",
      "Iteration 118/150\n",
      "Iteration 119/150\n",
      "Iteration 120/150\n",
      "Iteration 121/150\n",
      "Iteration 122/150\n",
      "Iteration 123/150\n",
      "Iteration 124/150\n",
      "Iteration 125/150\n",
      "Iteration 126/150\n",
      "Iteration 127/150\n",
      "Iteration 128/150\n",
      "Iteration 129/150\n",
      "Iteration 130/150\n",
      "Iteration 131/150\n",
      "Iteration 132/150\n",
      "Iteration 133/150\n",
      "Iteration 134/150\n",
      "Iteration 135/150\n",
      "Iteration 136/150\n",
      "Iteration 137/150\n",
      "Iteration 138/150\n",
      "Iteration 139/150\n",
      "Iteration 140/150\n",
      "Iteration 141/150\n",
      "Iteration 142/150\n",
      "Iteration 143/150\n",
      "Iteration 144/150\n",
      "Iteration 145/150\n",
      "Iteration 146/150\n",
      "Iteration 147/150\n",
      "Iteration 148/150\n",
      "Iteration 149/150\n",
      "Iteration 150/150\n",
      "Топ 10 слов по каждому тегу\n",
      "Topic  0: post | please | thank | would | email | know | send | anyone | list | question\n",
      "Topic  1: price | use | sell | buy | one | would | new | get | sale | offer\n",
      "Topic  2: maxaxaxaxaxaxaxaxaxaxaxaxaxaxax | bike | dod | ride | motorcycle | helmet | rid | rider | dog | bmw\n",
      "Topic  3: god | jesus | christian | one | believe | bible | say | church | religion | life\n",
      "Topic  4: car | use | get | wire | one | drive | power | light | good | engine\n",
      "Topic  5: armenian | turkish | book | greek | history | turkey | turk | university | genocide | armenia\n",
      "Topic  6: israel | war | jew | israeli | state | right | people | arab | country | attack\n",
      "Topic  7: president | new | work | say | state | american | program | year | april | group\n",
      "Topic  8: space | launch | system | nasa | satellite | data | science | center | mission | technology\n",
      "Topic  9: file | use | program | entry | line | bite | number | set | section | output\n",
      "Topic 10: get | good | would | think | one | dont | like | time | much | make\n",
      "Topic 11: window | use | file | program | available | version | run | server | software | get\n",
      "Topic 12: get | know | dont | think | would | like | say | go | one | see\n",
      "Topic 13: use | study | cause | medical | disease | drug | food | health | patient | doctor\n",
      "Topic 14: game | team | play | player | year | win | season | league | score | hockey\n",
      "Topic 15: say | one | go | people | come | take | know | tell | kill | woman\n",
      "Topic 16: one | would | say | make | think | people | question | dont | mean | point\n",
      "Topic 17: gun | law | state | right | crime | weapon | government | firearm | use | police\n",
      "Topic 18: key | use | chip | encryption | system | clipper | government | message | privacy | security\n",
      "Topic 19: drive | card | use | disk | scsi | system | driver | problem | do | window\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "newsgroups_train = fetch_20newsgroups(\n",
    "    subset='train',\n",
    "    remove=('headers', 'footers', 'quotes')\n",
    ")\n",
    "data = newsgroups_train.data\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def advanced_tokenizer(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if len(t) > 2 and t not in stop_words]\n",
    "    lemmas = []\n",
    "    for t in tokens:\n",
    "        lemma = lemmatizer.lemmatize(t)           \n",
    "        lemma = lemmatizer.lemmatize(lemma, 'v')   \n",
    "        lemma = lemmatizer.lemmatize(lemma, 'a')   \n",
    "        lemma = lemmatizer.lemmatize(lemma, 'r')   \n",
    "        lemma = lemmatizer.lemmatize(lemma, 's')   \n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "vectorizer = CountVectorizer(\n",
    "    tokenizer=advanced_tokenizer,\n",
    "    min_df=10,\n",
    "    max_df=0.4,\n",
    "    lowercase=False\n",
    ")\n",
    "X_train = vectorizer.fit_transform(data)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "D = X_train.shape[0]\n",
    "V = X_train.shape[1]\n",
    "docs_idx = []\n",
    "words_idx = []\n",
    "for d in range(D):\n",
    "    row = X_train[d]\n",
    "    for word_id, count in zip(row.indices, row.data):\n",
    "        docs_idx.extend([d] * count)\n",
    "        words_idx.extend([word_id] * count)\n",
    "\n",
    "docs_idx = np.array(docs_idx, dtype=int)\n",
    "words_idx = np.array(words_idx, dtype=int)\n",
    "W = len(docs_idx)\n",
    "K = 20\n",
    "alpha = 0.1\n",
    "beta = 0.01\n",
    "num_iter = 150     \n",
    "np.random.seed(42)\n",
    "z = np.random.randint(0, K, size=W)\n",
    "ndk = np.zeros((D, K))\n",
    "nkw = np.zeros((K, V))\n",
    "nk = np.zeros(K)\n",
    "for i in range(W):\n",
    "    d = docs_idx[i]\n",
    "    w = words_idx[i]\n",
    "    k = z[i]\n",
    "    ndk[d, k] += 1\n",
    "    nkw[k, w] += 1\n",
    "    nk[k] += 1\n",
    "beta_V = beta * V\n",
    "for iteration in range(num_iter):\n",
    "    print(f\"Iteration {iteration + 1}/{num_iter}\")\n",
    "    for i in range(W):\n",
    "        d = docs_idx[i]\n",
    "        w = words_idx[i]\n",
    "        old_k = z[i]\n",
    "        ndk[d, old_k] -= 1\n",
    "        nkw[old_k, w] -= 1\n",
    "        nk[old_k] -= 1\n",
    "        p = (ndk[d] + alpha) * (nkw[:, w] + beta) / (nk + beta_V)\n",
    "        p /= p.sum()\n",
    "        new_k = np.random.choice(K, p=p)\n",
    "        z[i] = new_k\n",
    "        ndk[d, new_k] += 1\n",
    "        nkw[new_k, w] += 1\n",
    "        nk[new_k] += 1\n",
    "phi = (nkw + beta) / (nk[:, np.newaxis] + beta_V)\n",
    "print(\"Топ 10 слов по каждому тегу\")\n",
    "for k in range(K):\n",
    "    top_idx = np.argsort(phi[k])[-10:][::-1]\n",
    "    top_words = [feature_names[i] for i in top_idx]\n",
    "    print(f\"Topic {k:2d}: {' | '.join(top_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36920dcb-9f79-4c84-a574-2427cdf08548",
   "metadata": {},
   "source": [
    "(код работал более 1.5 часов, поэтому я отслеживал итерации)\n",
    "Чистыми темами получились:\n",
    "Topic 1: misc.forsale\n",
    "Topic 3: soc.religion.christian\n",
    "Topic 4: rec.autos\n",
    "Topic 8: sci.space\n",
    "Topic 13: sci.med\n",
    "Topic 14: rec.sport.hockey\n",
    "Topic 17: talk.politics.guns\n",
    "Topic 18: sci.crypt\n",
    "Topic 19: comp.sys.ibm.pc.hardware\n",
    "\n",
    "Почти чистыми (есть общие или лишние или мусорные слова) получились:\n",
    "Topic 2: rec.motorcycles (есть мусорное слово maxaxaxa)\n",
    "Topic 5: talk.politics.mideast (была раскрыта лишь часть связанная с армяно турецким конфликтом)\n",
    "Topic 6: talk.politics.mideast (та же тема, но здесь про арабов и евреев, поэтому выделилась в отдельную тему)\n",
    "Topic 7: talk.politics.misc (есть общие слова)\n",
    "Topic 9: comp.graphics (пересечения с другими темами)\n",
    "Topic 11: comp.os.ms-windows.misc / comp.windows.x (сложно определить конкретно)\n",
    "Остальные в основном состоят из общих слов. \n",
    "(можно конечно уменьшить альфа и увеличить итерации, но тогда компьютер будет обрабатывать это более 2 часов) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36959065-310d-4d4d-9eb9-8b26f2af0a85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
